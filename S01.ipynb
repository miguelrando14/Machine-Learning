{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e774a6e-f147-4ee7-9b03-2050d0f65dab",
      "metadata": {
        "id": "6e774a6e-f147-4ee7-9b03-2050d0f65dab"
      },
      "source": [
        "# LAB 2: Feature Representation\n",
        "\n",
        "The goals of this lab are the following :\n",
        "- Making data linearly separable by utilizing polynomial features\n",
        "- Understanding the relation between model complexity and performance, underfitting, overfitting\n",
        "- Understanding how to deal with Categorical variables and preprocess features\n",
        "\n",
        "This lab is greatly inspired by those notes on feature representation, feel free to check it out : https://openlearninglibrary.mit.edu/assets/courseware/v1/b5ca509c17bab346cc6252ca41a1aac7/asset-v1:MITx+6.036+1T2019+type@asset+block/notes_chapter_Feature_representation.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a8645fc-488e-4ee0-8668-08d0f1fec02d",
      "metadata": {
        "id": "3a8645fc-488e-4ee0-8668-08d0f1fec02d"
      },
      "source": [
        "## Imports and helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b42985af-b69d-49e2-b7b9-4ef3e6bd70a6",
      "metadata": {
        "id": "b42985af-b69d-49e2-b7b9-4ef3e6bd70a6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Perceptron, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36279489-7365-4df2-9d2c-7eed75f5f47b",
      "metadata": {
        "id": "36279489-7365-4df2-9d2c-7eed75f5f47b"
      },
      "outputs": [],
      "source": [
        "def plot_2D_data(X, y, ax, axlims =None):\n",
        "    '''\n",
        "    Plot a 2D dataset\n",
        "    \n",
        "    Parameters :\n",
        "    ------------\n",
        "     X: np.ndarray, shape (num_samples, num_features)\n",
        "            An input data matrix, where each row is an input vector and each\n",
        "            column is a feature. num_features should be equal to two\n",
        "        \n",
        "     y: np.ndarray, shape (num_samples,)\n",
        "            The class label vector for the training data. Each element\n",
        "            corresponds to the class label for the corresponding row in X. All the labels should be 1 or -1\n",
        "    '''\n",
        "    num_samples, num_features = X.shape\n",
        "    assert num_features == 2, f'X should be of shape (num_samples, 2) but is actually of shape {X.shape}'\n",
        "    x1 = X[:, 0]\n",
        "    x2 = X[:, 1]\n",
        "    \n",
        "    \n",
        "    ax.scatter(x1[y==1], x2[y==1],c='b', edgecolors='k', linewidth=.5, label=\"1\")\n",
        "    ax.scatter(x1[y==-1], x2[y==-1],c='r', marker='x' ,edgecolors='k', linewidth=.5, label=\"-1\")\n",
        "    \n",
        "    if axlims is None:\n",
        "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "        xmin = xlim[0] -0.1 *(xlim[1] - xlim[0])\n",
        "        xmax = xlim[1] +0.1 *(xlim[1] - xlim[0])\n",
        "        ymin = ylim[0] -0.1 *(ylim[1] - ylim[0])\n",
        "        ymax = ylim[1] +0.1 *(ylim[1] - ylim[0])\n",
        "    else:\n",
        "        xmin, xmax, ymin, ymax = axlims\n",
        "    ax.set_xlim(xmin, xmax)\n",
        "    ax.set_ylim(ymin, ymax)\n",
        "    \n",
        "    \n",
        "    ax.set_title('Dataset')\n",
        "    ax.set_xlabel('$x_1$')\n",
        "    ax.set_ylabel('$x_2$')\n",
        "    ax.legend(loc='best')\n",
        "    return ax\n",
        "    \n",
        "def plot_2D_linear_decision_boundaries(X, y, coef, bias, ax, axlims=None):\n",
        "    ax = plot_2D_data(X, y, ax, axlims=axlims)\n",
        "    xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "    xx = np.linspace(xlim[0],xlim[1], 10)\n",
        "    if coef[1] != 0:\n",
        "        yy = - (coef[0] * xx + bias) / coef[1]\n",
        "        ax.plot(xx, yy, c='purple', label = 'Decision boundary')\n",
        "    elif coef[0] != 0:\n",
        "        plt.axvline(x = - coef[0] / bias, color='purple', label=f'Decision boundary')\n",
        "    ax.legend(loc='best')\n",
        "    return ax\n",
        "\n",
        "def plot_2D_polynomial_decision_boundaries(X, y, get_polynomial_features, k, coef, ax, axlims=None):\n",
        "    ax = plot_2D_data(X, y, ax, axlims=axlims)\n",
        "    xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "    u = np.linspace(xlim[0], xlim[1], 100)\n",
        "    v = np.linspace(ylim[0], ylim[1], 100)\n",
        "    U, V = np.meshgrid(u, v)\n",
        "    \n",
        "    \n",
        "    U = np.ravel(U)\n",
        "    V = np.ravel(V)\n",
        "    \n",
        "    X_poly = get_polynomial_features(np.stack([U,V], axis=1),k=k)\n",
        "    Z = np.dot(X_poly, coef)\n",
        "    U = U.reshape((len(u),len(v)))\n",
        "    V = V.reshape((len(u),len(v)))\n",
        "    Z = Z.reshape((len(u),len(v)))\n",
        "    db = ax.contour(U,V,Z, levels=[0], colors='purple')\n",
        "    db.collections[0].set_label('Decision Boundary')\n",
        "    ax.legend(loc='best')\n",
        "    clf_regions = ax.contourf(U,V,Z, levels=[Z.min()-1,0,Z.max()+1], colors=['r','b'], alpha=0.1)\n",
        "    return ax\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22d48df6-83d8-4205-a40e-0ce4578cff31",
      "metadata": {
        "id": "22d48df6-83d8-4205-a40e-0ce4578cff31"
      },
      "source": [
        "# Polynomial features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7524476-017a-4571-9ab8-25347e026b89",
      "metadata": {
        "id": "d7524476-017a-4571-9ab8-25347e026b89"
      },
      "source": [
        "We have seen in the previous Practical session that when the data is linearly separable, the perceptron algorithm converges and finds a linear separation of the data. However, when the data is not linearly separable, this algorithm might not be able to produce a meaningful result. Let's look at some examples here :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb2780f5-8ddc-49c1-95cb-6dfec20669ff",
      "metadata": {
        "id": "fb2780f5-8ddc-49c1-95cb-6dfec20669ff"
      },
      "source": [
        "## A linearly separable dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50fb1db2-873c-468a-a6ba-75a2e2e22774",
      "metadata": {
        "id": "50fb1db2-873c-468a-a6ba-75a2e2e22774"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(centers=2, random_state=10)\n",
        "y[y==0] =-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533d1459-3d4e-49d7-97b0-33573d079079",
      "metadata": {
        "id": "533d1459-3d4e-49d7-97b0-33573d079079"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_data(X,y,ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "590331a6-a044-454c-a0f1-42cf5c0ebf5c",
      "metadata": {
        "id": "590331a6-a044-454c-a0f1-42cf5c0ebf5c"
      },
      "outputs": [],
      "source": [
        "clf = Perceptron()\n",
        "clf.fit(X,y)\n",
        "coef, intercept = clf.coef_[0], clf.intercept_\n",
        "print(f'Coefficients : {coef}')\n",
        "print(f'bias : {intercept}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565d20a0-f6a6-4381-b478-e6c1085b800d",
      "metadata": {
        "id": "565d20a0-f6a6-4381-b478-e6c1085b800d"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_linear_decision_boundaries(X, y, coef, intercept, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21bdda06-18be-4972-a552-8e300e5718a1",
      "metadata": {
        "id": "21bdda06-18be-4972-a552-8e300e5718a1"
      },
      "source": [
        "## The XOR dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a48b6664-d4da-497b-9726-e07d8d0c8e0d",
      "metadata": {
        "id": "a48b6664-d4da-497b-9726-e07d8d0c8e0d"
      },
      "outputs": [],
      "source": [
        "X = np.array([[i,j] for i in[-1,1] for j in [-1,1]])\n",
        "y = X[:,0] * X[:,1]\n",
        "# Add some noise\n",
        "X = 1.0 * X +  0.01 * np.random.rand(4,2) \n",
        "print(f'The data points are :\\n {X}')\n",
        "print(f'Their corresponding labels are {y}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52a1719b-c357-4f1a-a513-a6e4ddc804cc",
      "metadata": {
        "id": "52a1719b-c357-4f1a-a513-a6e4ddc804cc"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_data(X, y, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e0382d-d72c-4f95-9a7f-ae816b4ae4b0",
      "metadata": {
        "id": "75e0382d-d72c-4f95-9a7f-ae816b4ae4b0"
      },
      "source": [
        "Q: Is this dataset linearly separable ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed69759-1842-41e8-b197-11a7786b83cc",
      "metadata": {
        "id": "6ed69759-1842-41e8-b197-11a7786b83cc"
      },
      "outputs": [],
      "source": [
        "clf = Perceptron()\n",
        "clf.fit(X,y)\n",
        "coef, intercept = clf.coef_[0], clf.intercept_\n",
        "print(f'Coefficients : {coef}')\n",
        "print(f'bias : {intercept}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e12bee54-24a3-4fd0-b192-3618dc36df68",
      "metadata": {
        "id": "e12bee54-24a3-4fd0-b192-3618dc36df68"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_linear_decision_boundaries(X, y, coef, intercept, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57629e98-3f4b-4289-886c-678006439529",
      "metadata": {
        "id": "57629e98-3f4b-4289-886c-678006439529"
      },
      "source": [
        "## A 1D dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b7bbb8e-66ab-4355-9126-e89976369f29",
      "metadata": {
        "id": "3b7bbb8e-66ab-4355-9126-e89976369f29"
      },
      "source": [
        "We consider a very simple 1D dataset with 4 datapoints. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c24d301e-e937-4ece-a293-d6ec6f5ac8b7",
      "metadata": {
        "id": "c24d301e-e937-4ece-a293-d6ec6f5ac8b7"
      },
      "outputs": [],
      "source": [
        "X = np.array([-3, -1, 1.25, 2])\n",
        "# We add a second coordinate equal to 0 for easier visualization\n",
        "X_aug_0 = np.stack([X, np.zeros_like(X)], axis=1)\n",
        "y = np.array([1, -1, -1, 1])\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_data(X_aug_0, y, ax)\n",
        "ax.set_ylabel('$x_2 = 0$')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3202f698-9180-4a0d-9608-ae67637732b7",
      "metadata": {
        "id": "3202f698-9180-4a0d-9608-ae67637732b7"
      },
      "source": [
        "Q: What is a linear boundary for a 1D dataset ?\n",
        "\n",
        "Q: Is this dataset Linearly separable ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6a18e22-86a5-4c4f-9a21-bdf0695aa664",
      "metadata": {
        "id": "c6a18e22-86a5-4c4f-9a21-bdf0695aa664"
      },
      "source": [
        "### NaÃ¯ve Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b15349-cc7e-41cf-a4fc-07eb4bcd27de",
      "metadata": {
        "id": "90b15349-cc7e-41cf-a4fc-07eb4bcd27de"
      },
      "outputs": [],
      "source": [
        "clf = Perceptron()\n",
        "clf.fit(X_aug_0,y)\n",
        "coef0, intercept0 = clf.coef_[0], clf.intercept_\n",
        "print(f'Coefficients : {coef0}')\n",
        "print(f'bias : {intercept0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5571c0f4-bf7c-40a3-9884-b18d80b26930",
      "metadata": {
        "id": "5571c0f4-bf7c-40a3-9884-b18d80b26930"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_linear_decision_boundaries(X_aug_0, y, coef0, intercept0, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1006c559-0c5e-44db-81fa-dadb19dd061d",
      "metadata": {
        "id": "1006c559-0c5e-44db-81fa-dadb19dd061d"
      },
      "source": [
        "### A smarter way to proceed "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98bb14f9-07fc-4dd3-938e-82b91a12e88e",
      "metadata": {
        "id": "98bb14f9-07fc-4dd3-938e-82b91a12e88e"
      },
      "source": [
        "What if we could put the data in a 2D space in a way that would make the data linearly separable ? One way is to consider the transformation $\\Phi(x) = (x, x^2)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aedab4d-c00c-41b2-b65d-4c139c346df6",
      "metadata": {
        "id": "8aedab4d-c00c-41b2-b65d-4c139c346df6"
      },
      "outputs": [],
      "source": [
        "X_aug_poly = np.stack([X, X**2], axis= 1)\n",
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_data(X_aug_poly, y, ax)\n",
        "ax.set_ylabel('$x^2$')\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d71d31e-7410-4062-9aaf-d8eaf09726f3",
      "metadata": {
        "id": "6d71d31e-7410-4062-9aaf-d8eaf09726f3"
      },
      "source": [
        "Q : Is this new dataset lineary separable ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85fc08d7-1fa4-4c6d-960f-ce46620ecd0b",
      "metadata": {
        "id": "85fc08d7-1fa4-4c6d-960f-ce46620ecd0b"
      },
      "outputs": [],
      "source": [
        "# TODO Train a perceptron on the augmented dataset\n",
        "# coef_poly =\n",
        "# intercept_poly = \n",
        "# print(f'Coefficients : {coef_poly}')\n",
        "# print(f'bias : {intercept_poly}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1240d92f-1563-4d89-ad59-c44bd502353a",
      "metadata": {
        "id": "1240d92f-1563-4d89-ad59-c44bd502353a"
      },
      "outputs": [],
      "source": [
        "# plot the resulting boundary\n",
        "_, axes = plt.subplots(1,2, figsize=(10,5))\n",
        "plt.subplots_adjust(right=1.5, top=1.5)\n",
        "#Plot new separation\n",
        "axes[0] = plot_2D_linear_decision_boundaries(X_aug_poly, y, coef_poly, intercept_poly, axes[0])\n",
        "axes[0].set_xlabel('x')\n",
        "axes[0].set_ylabel('$x^2$')\n",
        "axes[0].set_title('Polynomial Features')\n",
        "\n",
        "#Plot old separation\n",
        "axes[1] = plot_2D_linear_decision_boundaries(X_aug_0, y, coef0, intercept0, axes[1])\n",
        "axes[1].set_xlabel('x')\n",
        "axes[1].set_ylabel('$x_2 = 0$')\n",
        "axes[1].set_title('Basic Features')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92cdc7d1-0c50-4fe2-ac1a-d1a06d0f6384",
      "metadata": {
        "id": "92cdc7d1-0c50-4fe2-ac1a-d1a06d0f6384"
      },
      "source": [
        "*Tips :* plt.subplots(n_rows, n_columns) allows you to easily create multiple plots on the same figure !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da3a155e-1438-4ac5-9680-036b5ce274fa",
      "metadata": {
        "id": "da3a155e-1438-4ac5-9680-036b5ce274fa"
      },
      "source": [
        "Q : Represent the decision Boundary on the 1D Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db621a5f-7d6d-4297-8281-b6dd94ff2304",
      "metadata": {
        "id": "db621a5f-7d6d-4297-8281-b6dd94ff2304"
      },
      "source": [
        "## Polynomial Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ce301c8-18de-4010-b865-790e481218d4",
      "metadata": {
        "id": "0ce301c8-18de-4010-b865-790e481218d4"
      },
      "source": [
        "If your dataset has $D$ numerical features, a systematic strategy is to use a polynomial basis. The polynomial basis of order $k$ (where $k$ is a positive integer) contains all products of features of degree inferior or equal to $k$.\n",
        "\n",
        "For example the monome $x_1x_2^3$ is of degree $4$ and the monome $x_1x_2x_3^4$ is of degree 6\n",
        "\n",
        "The following table illustrates the $k$-th order polynomial basis for different values of $k$:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e418af-36d7-4d5c-aa9f-c39e272aab31",
      "metadata": {
        "id": "f3e418af-36d7-4d5c-aa9f-c39e272aab31"
      },
      "source": [
        "\\begin{array}{|c|c|c|}\n",
        "\\hline \\\\\n",
        "\\text{Order} k & D=1 & D>1 \\\\\n",
        "\\hline \\\\\n",
        "0 &[1] & [1]\\\\\n",
        "1 & [1,x]&[1,x_1,\\ldots,x_d] \\\\\n",
        "2 & [1,x,x^2] & [1,x_1,\\ldots,x_d, x_1^2,x_1x_2,\\ldots]\\\\\n",
        "3 & [1,x,x^2,x^3]& [1,x_1,\\ldots,x_d, x_1^2,x_1x_2,\\ldots, x_1^3,x_1^2x_2,x_1,x_2,x_3,\\ldots]\\\\ \n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "\\end{array}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bce3018-11b5-4ead-ade3-9298e4422428",
      "metadata": {
        "id": "7bce3018-11b5-4ead-ade3-9298e4422428"
      },
      "source": [
        "For example, if you consider a polynomial basis or order 2 for D=2 features, the corresponding polynomial features are $[1,x_1,x_2,x_1^2,x_1x_2,x_2^2]$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf35c316-11b5-429c-8c12-a96fde1b48c4",
      "metadata": {
        "id": "cf35c316-11b5-429c-8c12-a96fde1b48c4"
      },
      "source": [
        "Bonus Question : If you have 2 features (D=2), how many features are there in the $k$-th order polynomial basis? Same question for a general $D$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bb4acdd-eb51-4ea3-8bcc-f2a48b6354f8",
      "metadata": {
        "id": "1bb4acdd-eb51-4ea3-8bcc-f2a48b6354f8"
      },
      "source": [
        "Here, you should code the `get_polynomial_features` function that will create the polynomial basis of order $k$ for a 2D dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2172af-470e-410f-a3a7-f1d093eb56c5",
      "metadata": {
        "id": "ae2172af-470e-410f-a3a7-f1d093eb56c5"
      },
      "outputs": [],
      "source": [
        "# Generate a random dataset of size(10,2)\n",
        "X = np.random.rand(10,2)\n",
        "\n",
        "def get_polynomial_features(X, k=2):\n",
        "    \"\"\"\n",
        "    Creates the polynomial features for a 2D dataset X.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X: np.ndarray of shape (num_samples, 2)\n",
        "    An input data matrix where each row is an input vector and each column is a feature.\n",
        "    \n",
        "    k : int\n",
        "    The order of the polynomial basis\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_aug: np.ndarray of shape (num_samples, dimension of polynomial basis of order k)\n",
        "    An augmented data matrix containing all the polynomial features of the polynomial basis of order k\n",
        "    \"\"\"\n",
        "    \n",
        "    ### TO CODE\n",
        "    return X_aug\n",
        "\n",
        "X_aug = get_polynomial_features(X, k=2)\n",
        "print(f'The original dataset was of shape {X.shape}')\n",
        "print(f'The augmented dataset is of shape {X_aug.shape}')\n",
        "assert X_aug.shape[1] == 6, f'X_aug is of shape {X_aug.shape} but there should be 6 features for a polynomial basis of order 2 with 2 original features'\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5c637cd-984c-4b4f-8373-0ba7309fe4dc",
      "metadata": {
        "id": "b5c637cd-984c-4b4f-8373-0ba7309fe4dc"
      },
      "source": [
        "## Back to the XOR dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9412e65a-1d9b-428b-aba7-267b107f23a8",
      "metadata": {
        "id": "9412e65a-1d9b-428b-aba7-267b107f23a8"
      },
      "outputs": [],
      "source": [
        "X = np.array([[i,j] for i in[-1,1] for j in [-1,1]])\n",
        "y = X[:,0] * X[:,1]\n",
        "# Add some noise\n",
        "X = 1.0 * X +  0.01 * np.random.rand(4,2) \n",
        "print(f'The data points are :\\n {X}')\n",
        "print(f'Their corresponding labels are {y}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf01b641-b610-4066-96f8-59fab576246b",
      "metadata": {
        "id": "bf01b641-b610-4066-96f8-59fab576246b"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_data(X, y, ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46904885-900d-4ff8-9b5a-ac842ad4733b",
      "metadata": {
        "id": "46904885-900d-4ff8-9b5a-ac842ad4733b"
      },
      "outputs": [],
      "source": [
        "clf = Perceptron()\n",
        "clf.fit(X,y)\n",
        "coef0, intercept0 = clf.coef_[0], clf.intercept_\n",
        "print(f'Coefficients : {coef}')\n",
        "print(f'bias : {intercept}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99092cc1-9905-442f-b824-dd0395981633",
      "metadata": {
        "id": "99092cc1-9905-442f-b824-dd0395981633"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots()\n",
        "ax = plot_2D_linear_decision_boundaries(X, y, coef, intercept, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3b312b-a416-419d-8bf8-7667e75f84bb",
      "metadata": {
        "id": "2f3b312b-a416-419d-8bf8-7667e75f84bb"
      },
      "source": [
        "We are now going to make this dataset linearly separable by transforming it into a higher dimensional dataset using our polynomial features. We can then use the perceptron algorithm on the augmented dataset to solve the classification problem. Let's try it with a polynomial basis of order $k=2$. In that case the feature transformation is $$\\phi([x_1,x_2]) =[1,x_1,x_2,x_1^2,x_1x_2,x_2^2]$$\n",
        "\n",
        "Q : With that feature transformation, do we need a bias in the perceptron algorithm ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b455ca4e-e9bd-4e36-b5a2-7d680fab74a2",
      "metadata": {
        "id": "b455ca4e-e9bd-4e36-b5a2-7d680fab74a2"
      },
      "outputs": [],
      "source": [
        "# TODO : Add polynomials feature to the dataset\n",
        "# X_aug =\n",
        "\n",
        "# TODO : Train a perceptron on the augmented dataset\n",
        "# clf = Perceptron(fit_intercept=False)\n",
        "# coef_XOR = \n",
        "\n",
        "# Plot the resulting decision boundary\n",
        "_, axes = plt.subplots(1,2, figsize = (10,5))\n",
        "\n",
        "#Plot Separation with polynomial Features\n",
        "axes[0] = plot_2D_polynomial_decision_boundaries(X, y, get_polynomial_features, k=2, coef=coef_XOR, ax= axes[0])\n",
        "axes[0].set_title('Polynomial Features')\n",
        "\n",
        "#Plot Separation without Polynomial Features\n",
        "axes[1] = plot_2D_linear_decision_boundaries(X, y, coef0, intercept0, axes[1])\n",
        "axes[1].set_xlabel('x')\n",
        "axes[1].set_title('Basic Features')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff8a04e-8f8e-4b1e-b1cf-da13a31a82bf",
      "metadata": {
        "id": "bff8a04e-8f8e-4b1e-b1cf-da13a31a82bf"
      },
      "source": [
        "## The Moon Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f17d85cb-dd5e-4a28-9c49-920cf905a2ce",
      "metadata": {
        "id": "f17d85cb-dd5e-4a28-9c49-920cf905a2ce"
      },
      "source": [
        "We now move our focus to another non linearly separable dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4761a3a1-98fd-4a63-be38-05dbe81eb972",
      "metadata": {
        "id": "4761a3a1-98fd-4a63-be38-05dbe81eb972"
      },
      "outputs": [],
      "source": [
        "X,y = make_moons(n_samples = 400, noise =0.1, random_state=42,)\n",
        "y[y!=1] = -1\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ax = plot_2D_data(X, y, ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4cae629-dc3a-40a9-a9ca-1f1fc46eb05d",
      "metadata": {
        "id": "c4cae629-dc3a-40a9-a9ca-1f1fc46eb05d"
      },
      "outputs": [],
      "source": [
        "# TODO Compute the polynomial features of order k, fit a perceptron, compute the number of errors and plot the associated decision boundary for k taking values in [1,2,3,4,5,6]. \n",
        "_, axes = plt.subplots(2,3, figsize = (15,10))\n",
        "plt. subplots_adjust(left=None, bottom=None, right=1.5, top=1.5, wspace=None, hspace=None)\n",
        "list_degrees = [1,2,3,4,5,6]\n",
        "for k, ax in zip(list_degrees, axes.flatten()):\n",
        "    # TODO Compute the polynomial features of order k\n",
        "    \n",
        "    # TODO Fit a Perceptron to the polynomial features and get the coefficients\n",
        "    # coef_moon =\n",
        "    \n",
        "    # TODO Compute the number of errors\n",
        "    # n_errors = \n",
        "    ax = plot_2D_polynomial_decision_boundaries(X, y, get_polynomial_features, k=k, coef=coef_moon, ax= ax)\n",
        "    ax.set_title(f'Polynomial Features of order {k}\\n Number of errors : {n_errors}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f59283-c449-454b-93ef-62d8ff90e9d0",
      "metadata": {
        "id": "25f59283-c449-454b-93ef-62d8ff90e9d0"
      },
      "source": [
        "Q : Which value of $k$ should we choose here? Why ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2123cb8-36c1-4456-819b-00812319ea42",
      "metadata": {
        "id": "b2123cb8-36c1-4456-819b-00812319ea42"
      },
      "source": [
        "## An Introduction to Generalization error "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805f4345-fdad-4a43-a43f-650e4bf85de3",
      "metadata": {
        "id": "805f4345-fdad-4a43-a43f-650e4bf85de3"
      },
      "source": [
        "In Machine Learning, usually the objective is not only to fit existing data with our model but also to make good predictions on new data that the model has never seen before. We take a look at the following blob datasets :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018bee68-db15-44c8-a3b4-5a58c9f2b480",
      "metadata": {
        "id": "018bee68-db15-44c8-a3b4-5a58c9f2b480"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples = 120, centers=2, cluster_std=5, random_state=48)\n",
        "y[y!=1] = -1\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ax = plot_2D_data(X, y, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0541c22b-d46d-49df-afff-b86c207fa05b",
      "metadata": {
        "id": "0541c22b-d46d-49df-afff-b86c207fa05b"
      },
      "source": [
        "Here we will use another model we will learn about in future lectures, logistic regression, that performs better than the Perceptron algorithm on non Linearly separable data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cabc41-5fcc-41a3-a6ec-0bf1e33fab20",
      "metadata": {
        "id": "b1cabc41-5fcc-41a3-a6ec-0bf1e33fab20"
      },
      "outputs": [],
      "source": [
        "# TODO Compute the polynomial features of order k, fit a Logistic Regression, compute the number of errors and plot the associated decision boundary for k taking values in [1,2,3,4,5,6]. \n",
        "_, axes = plt.subplots(2,3, figsize = (15,10))\n",
        "plt. subplots_adjust(left=None, bottom=None, right=1.5, top=1.5, wspace=None, hspace=None)\n",
        "list_degrees = [1,2,3,4,5,6]\n",
        "for k, ax in zip(list_degrees, axes.flatten()):\n",
        "    # TODO Compute the polynomial features of order k\n",
        "    \n",
        "    # TODO Fit a Logistic Regression to the polynomial features and get the coefficients\n",
        "    # clf = LogisticRegression(fit_intercept=False, max_iter=10000, random_state=42)\n",
        "    # coef_blob =\n",
        "    \n",
        "    # TODO Compute the number of errors\n",
        "    # n_errors = \n",
        "    ax = plot_2D_polynomial_decision_boundaries(X, y, get_polynomial_features, k=k, coef=coef_blob, ax= ax)\n",
        "    ax.set_title(f'Polynomial Features of order {k}\\n Number of errors : {n_errors}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54a4dccf-e1ac-48d1-8ca5-3e2d9753af0e",
      "metadata": {
        "id": "54a4dccf-e1ac-48d1-8ca5-3e2d9753af0e"
      },
      "source": [
        "Q : Keeping in mind that we want our model to generalize well (meaning making relevant predictions on data points that we have not seen yet but that follow the same distribution that the data points we have seen), what do you think is the best value of $k$ to pick here?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ee217f-54d8-44e0-81dc-0f99b742226f",
      "metadata": {
        "id": "14ee217f-54d8-44e0-81dc-0f99b742226f"
      },
      "source": [
        "A very useful tool to measure the generalization of a model is to use a training set and a validation set. We train the model on the training set and then evaluate its performance on the validation set (with new data different than the one used during training). Here we split the data into Training set and Validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684d7776-07bd-4bad-aa69-193c3557395f",
      "metadata": {
        "id": "684d7776-07bd-4bad-aa69-193c3557395f"
      },
      "outputs": [],
      "source": [
        "# Get the min and max values of the data :\n",
        "xmin = X[:,0].min() - 0.1*(X[:,0].max()-X[:,0].min())\n",
        "xmax = X[:,0].max() + 0.1*(X[:,0].max()-X[:,0].min())\n",
        "ymin = X[:,1].min() - 0.1*(X[:,1].max()-X[:,1].min())\n",
        "ymax = X[:,1].max() + 0.1*(X[:,1].max()-X[:,1].min())\n",
        "axlims=(xmin, xmax, ymin, ymax)\n",
        "# Split the data into two groups, training and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.33, random_state=42)\n",
        "\n",
        "_, axes = plt.subplots(1,3, figsize = (15,5))\n",
        "for ax, X_sub, y_sub, name in zip(axes, [X, X_train, X_val], [y, y_train,y_val], ['Full set', 'Training set', 'Validation set']):\n",
        "    ax = plot_2D_data(X_sub, y_sub, ax,axlims=axlims)\n",
        "    ax.set_title(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5920dd-9f2f-4b48-a558-7073008501f0",
      "metadata": {
        "id": "8f5920dd-9f2f-4b48-a558-7073008501f0"
      },
      "outputs": [],
      "source": [
        "# TODO Compute the polynomial features of order k, fit a perceptron on the training set and plot the associated decision boundary for k taking values in [1,2,3,4,5,6] for both the training and validation sets.\n",
        "list_degrees = [1,2,3,4,5,6]\n",
        "_, axes = plt.subplots(len(list_degrees),2, figsize = (15,15))\n",
        "plt.subplots_adjust(left=None, bottom=None, right=1.5, top=1.5, wspace=None, hspace=None)\n",
        "for i,k in enumerate(list_degrees):\n",
        "    # TODO Compute the polynomial features of order k for both Training and validation sets\n",
        "    \n",
        "    # TODO On the training set, fit a Logistic Regression to the polynomial features and get the coefficients\n",
        "    # coef =\n",
        "    \n",
        "    # TODO Compute the number of errors for both training and validation\n",
        "    # n_errors_train =\n",
        "    # n_errors_valid =\n",
        "    axes[i,0] = plot_2D_polynomial_decision_boundaries(X_train, y_train, get_polynomial_features, k=k, coef=coef, ax= axes[i,0],axlims=axlims)\n",
        "    axes[i,0].set_title(f'Training set: Polynomial Features of order {k}\\n Number of training errors : {n_errors_train}')\n",
        "    axes[i,1] = plot_2D_polynomial_decision_boundaries(X_val, y_val, get_polynomial_features, k=k, coef=coef, ax= axes[i,1],axlims=axlims)\n",
        "    axes[i,1].set_title(f'Validation set: Polynomial Features of order {k}\\n Number of validation errors : {n_errors_valid}') "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bda5e79-a720-4769-9607-d6c58b7fc9d1",
      "metadata": {
        "id": "3bda5e79-a720-4769-9607-d6c58b7fc9d1"
      },
      "source": [
        "Q : Using this new plot, which is the best value of $k$? Why ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f927ac17-7411-4997-80ce-5da4b085338a",
      "metadata": {
        "id": "f927ac17-7411-4997-80ce-5da4b085338a"
      },
      "source": [
        "# Categorical Variables and Feature preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a03918d-c064-4d9f-961a-c59c1aafb655",
      "metadata": {
        "id": "8a03918d-c064-4d9f-961a-c59c1aafb655"
      },
      "source": [
        "Here we will take a look at the dataset from the **Titanic - Machine Learning from Disaster** Dataset from the following Kaggle competition: https://www.kaggle.com/competitions/titanic/data  The goal of this competition is to use machine learning to predict the survival or death of passengers using available features(such as gender, age, ...). We strongly recommend that you try making a submission to this competition as an exercise, it's a really nice way to get your hands on real data and see more of the machine learning pipeline. We recommend this notebook to get started (https://www.kaggle.com/code/alexisbcook/titanic-tutorial/notebook). You might want to come back to the competition from time to time when you see new methods in class to see how they improve your results!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c43f53e-d75e-4223-b6ba-4c6b2d6d148d",
      "metadata": {
        "id": "2c43f53e-d75e-4223-b6ba-4c6b2d6d148d"
      },
      "source": [
        "You should upload to the google colab (or your local machine) the files `gender_submission.csv`, `train.csv`, test.csv that you can find on Aula Global."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "921f8325-dc50-4306-91e4-326576dffcd8",
      "metadata": {
        "id": "921f8325-dc50-4306-91e4-326576dffcd8"
      },
      "source": [
        "To deal with `.csv` files, a very useful `Python` library is `Pandas`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c385ffc-e0fe-4068-9ee4-e4289e4d0266",
      "metadata": {
        "id": "2c385ffc-e0fe-4068-9ee4-e4289e4d0266"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ed3c413-a2a4-4fb1-b9ed-514b81edf044",
      "metadata": {
        "id": "0ed3c413-a2a4-4fb1-b9ed-514b81edf044"
      },
      "source": [
        "We start by reading the different files, we can use the read_csv method to obtain a Dataframe a python object containing the content of the csv file. We can use the **head** method to see the first rows of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067e1494-eaf5-4904-9c8b-677c9221456f",
      "metadata": {
        "id": "067e1494-eaf5-4904-9c8b-677c9221456f"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944e0661-da74-4315-a2c1-28d1bb7265c1",
      "metadata": {
        "id": "944e0661-da74-4315-a2c1-28d1bb7265c1"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "197724ca-33eb-43e6-9916-6956e7b2b848",
      "metadata": {
        "id": "197724ca-33eb-43e6-9916-6956e7b2b848"
      },
      "source": [
        "As you can see, the `test.csv` file contains the information of passengers for which you want to make a prediction. The Survived column is not present is this `dataFrame`, indeed the goal of our model is to make prediction on data for which it doesn't know the target values, data that we can't use to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a3b815-c80a-4a05-a4c5-dbe8bc9dfa29",
      "metadata": {
        "id": "56a3b815-c80a-4a05-a4c5-dbe8bc9dfa29"
      },
      "outputs": [],
      "source": [
        "gender_submission = pd.read_csv('gender_submission.csv')\n",
        "gender_submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f97ac13e-9911-479d-a435-021e28403b96",
      "metadata": {
        "id": "f97ac13e-9911-479d-a435-021e28403b96"
      },
      "source": [
        "The gender_submission `dataFrame` contains a prediction on the test set that predicts that every woman survived and every men died. We can look at the training data to see the survival rate of men and women on the titanic :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ed09f6-fb44-435b-8f3b-c8d821e391f8",
      "metadata": {
        "id": "41ed09f6-fb44-435b-8f3b-c8d821e391f8"
      },
      "outputs": [],
      "source": [
        "print(f\"Rate of survival of men : {train[train.Sex == 'male'].Survived.mean()}\")\n",
        "print(f\"Rate of survival of women : {train[train.Sex == 'female'].Survived.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb9020a9-be9a-4ace-b9f8-c69a964a6001",
      "metadata": {
        "id": "cb9020a9-be9a-4ace-b9f8-c69a964a6001"
      },
      "source": [
        "As we can see, this is not a bad first guess for survival chance, but we can make a more educated guess using the other features of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6ef55f-40c6-4138-bfa4-dec643459608",
      "metadata": {
        "id": "cd6ef55f-40c6-4138-bfa4-dec643459608"
      },
      "source": [
        "So far, the models that we have seen are only able to deal with numerical variables. Here, only the variables **Age** and **Fare** are numerical. In order to use a perceptron or a logistic regression that takes into account the other variables, we would need to convert them to numerical variables. Usually a discrete variables takes one of k discrete values. Here are a few possible ways to encode them to numerical variables :\n",
        "- **Numeric** : Assign each of the values to a number, say 1.0/k, 2.0/k,..., 1.0. This is a reasonable choice only when the discrete values do signify some sort of numeric quantities.\n",
        "- **Thermometer code** : If your discrete values are ordered, from 1,...,k but not a natural mapping to real numbers, you can use a vector of length k binary variables, where we convert a discrete value $0<j\\leq k$ into a vector in which the first j values are 1.0 and the rest are 0.0. This convey something about the ordering.\n",
        "- **Factored code** : If your discrete value can be decomposed into two(ore more part), it can be best to treat those parts as separate features and choose an appropriate encoding for each of them.\n",
        "- **One-hot encoding** : The best strategy when there is no information on the data is to use a vector of length k where we convert a discrete value $0<j\\leq k$ into a vector for which all values are 0 except for the j-th which is 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "434cbff3-9340-4957-b999-5f752f4521ce",
      "metadata": {
        "id": "434cbff3-9340-4957-b999-5f752f4521ce"
      },
      "source": [
        "Q : If you are using a linear model, which of the Thermometer code and the One-hot encoding is more expressive ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252db867-ac6c-4bf2-b839-b5ee9887215e",
      "metadata": {
        "id": "252db867-ac6c-4bf2-b839-b5ee9887215e"
      },
      "source": [
        "We restrict our attention to a limited_set of features :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ca5aea-1c13-4afa-88ba-16e9190108b6",
      "metadata": {
        "id": "41ca5aea-1c13-4afa-88ba-16e9190108b6"
      },
      "outputs": [],
      "source": [
        "df = train.loc[:,['Pclass', 'Sex', 'Age', 'Fare','Embarked','Survived']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70508c35-4467-4ac0-a817-fa95db664295",
      "metadata": {
        "id": "70508c35-4467-4ac0-a817-fa95db664295"
      },
      "source": [
        "Sometimes, there are missing values in the dataset, this is not the topic of todays lecture, but here is an example of how to deal with them :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae3451a-277c-4751-aca9-7cafadd56aca",
      "metadata": {
        "id": "0ae3451a-277c-4751-aca9-7cafadd56aca"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2149eb7-7011-48fc-80a3-834fd73dde9f",
      "metadata": {
        "id": "e2149eb7-7011-48fc-80a3-834fd73dde9f"
      },
      "source": [
        "Here we can see that the value of the Age is missing for 177 passengers, we replace it by the mean value of the age of the passenger.(This is a solution, but not the only one). The value of Embarked is missing for two passengers, we select the most common value for them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d6ed17-2023-42f9-82d8-8d63079aadbb",
      "metadata": {
        "id": "77d6ed17-2023-42f9-82d8-8d63079aadbb"
      },
      "outputs": [],
      "source": [
        "df.Age.fillna(df.Age.mean(),inplace=True)\n",
        "df.Embarked.fillna(df.Embarked.loc[df.Embarked.value_counts().argmax()], inplace=True)\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7f692d-ba12-4300-9431-cc20a50c5da6",
      "metadata": {
        "id": "ee7f692d-ba12-4300-9431-cc20a50c5da6"
      },
      "source": [
        "Now there are no more missing values and we can focus on dealing with the categorical variables\n",
        "Here is a brief description of the variables of the dataframe :\n",
        "- Pclass : The class of the ticket, it can be 1rd, 2nd or 3rd\n",
        "- Sex : The sex of the Passenger\n",
        "- Age : Age in Years\n",
        "- Fare : Passenger Fare\n",
        "- embarked : Port of Embarkation(C = Cherbourg, Q = Queenstown, S= Southampton)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c901d5e-3bd3-4b69-81d6-4c5a9f2c3a39",
      "metadata": {
        "id": "4c901d5e-3bd3-4b69-81d6-4c5a9f2c3a39"
      },
      "source": [
        "TODO : For each of discrete variables, choose and implement an encoding of your choice\n",
        "PS : make sure the output is of shape(num_samples, encoding_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0c3552-e483-4f2b-b649-651b0fc216fb",
      "metadata": {
        "id": "0f0c3552-e483-4f2b-b649-651b0fc216fb"
      },
      "outputs": [],
      "source": [
        "# Encoding of the Passenger class\n",
        "# Pclass = df.Pclass.values\n",
        "\n",
        "# Pclass_enc ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd9288e-0382-49be-afe5-fce47ecce07f",
      "metadata": {
        "id": "afd9288e-0382-49be-afe5-fce47ecce07f"
      },
      "outputs": [],
      "source": [
        "#Encoding of the sex\n",
        "# Sex = df.Sex.values\n",
        "\n",
        "# Sex_enc ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a40d61f-eb87-4f83-860d-446add4c4d7d",
      "metadata": {
        "id": "9a40d61f-eb87-4f83-860d-446add4c4d7d"
      },
      "outputs": [],
      "source": [
        "# Encoding of the Port of Embarkation\n",
        "# embarked = df.embarked.values\n",
        "\n",
        "#embarked_enc ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eca16b4-7922-448f-9bca-40a8289fcd78",
      "metadata": {
        "id": "7eca16b4-7922-448f-9bca-40a8289fcd78"
      },
      "outputs": [],
      "source": [
        "# Obtain the numerical features and the target from the dataframe\n",
        "X = df.loc[:,['Age', 'Fare']].values\n",
        "y = df.loc[:,'Survived'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41739d69-cf38-49cb-bc0a-c619ea53471b",
      "metadata": {
        "id": "41739d69-cf38-49cb-bc0a-c619ea53471b"
      },
      "outputs": [],
      "source": [
        "# Add the encoded numerical variables\n",
        "X_aug = np.hstack([X, Pclass_enc, Sex_enc, embarked_enc])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71877f6-12ea-437f-9622-d6cbda4f69f3",
      "metadata": {
        "id": "e71877f6-12ea-437f-9622-d6cbda4f69f3"
      },
      "source": [
        "In order to evaluate the quality of our model, we could use it to make predictions on the test set, but we don't have access to the true values on this set (we could make a prediction on Kaggle to get a scoring). As before in this seminar, one way to get around this is to split the training data into training and validation data to evaluate the performance of the model on data it has never seen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae994d09-6fc1-452c-89b4-69afaf674a13",
      "metadata": {
        "id": "ae994d09-6fc1-452c-89b4-69afaf674a13"
      },
      "outputs": [],
      "source": [
        "id_train, id_val, y_train, y_val = train_test_split(list(range(len(X))),y,train_size=0.8,  random_state=44)\n",
        "X_train = X[id_train]\n",
        "X_val = X[id_val]\n",
        "print(f' Training set size : {len(id_train)}\\n Validation set size : {len(id_val)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6bea76-d371-457e-8892-c3759b8acb0a",
      "metadata": {
        "id": "7b6bea76-d371-457e-8892-c3759b8acb0a"
      },
      "outputs": [],
      "source": [
        "#X_train_aug = X_aug[id_train]\n",
        "#X_val_aug = X_aug[id_val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13369838-ed21-4257-b971-ff42d3016c61",
      "metadata": {
        "id": "13369838-ed21-4257-b971-ff42d3016c61"
      },
      "outputs": [],
      "source": [
        "# Accuracy of the Logistic Regression on the numerical variables\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_val)\n",
        "print(f'The accuracy of the logistic regression with the two numerical variables on the validation set is {(y_pred == y_val).mean()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a74bb3-a856-4420-9e78-ab92c48a53a1",
      "metadata": {
        "id": "87a74bb3-a856-4420-9e78-ab92c48a53a1"
      },
      "outputs": [],
      "source": [
        "# Accuracy of the gender submission\n",
        "y_pred = (df.Sex=='female').values[id_val]\n",
        "print(f'The accuracy of the logistic regression with the two numerical variables on the validation set is {(y_pred == y_val).mean()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff4de8e3-5be2-44d9-b662-a006f47f4c8c",
      "metadata": {
        "id": "ff4de8e3-5be2-44d9-b662-a006f47f4c8c"
      },
      "outputs": [],
      "source": [
        "# Accuracy of the augmented model\n",
        "clf = LogisticRegression(max_iter=300)\n",
        "clf.fit(X_train_aug, y_train)\n",
        "y_pred = clf.predict(X_val_aug)\n",
        "print(f'The accuracy of the logistic regression with the augmented model on the validation set is {(y_pred == y_val).mean()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d3e63c5-093f-421d-bbce-44f26b23a05e",
      "metadata": {
        "id": "0d3e63c5-093f-421d-bbce-44f26b23a05e"
      },
      "source": [
        "Q : Should you do any preprocessing on the numerical variables ? Propose two different ideas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b316fd0-2460-41d9-b1be-a89effeea92a",
      "metadata": {
        "id": "5b316fd0-2460-41d9-b1be-a89effeea92a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:crystal_clear]",
      "language": "python",
      "name": "conda-env-crystal_clear-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}