{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 3: LOGISTIC REGRESSION AND GRADIENT DESCENT\n",
        "\n",
        "The goals of this lab are the following:\n",
        "\n",
        "*   To understand Logistic Regression (LR).\n",
        "*   To analytically work out a small example of LR\n",
        "*   To learn how to code gradient descent and study its convergence in terms of the learning rate.\n",
        "*   To review overfitting in the context of LR\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tTYfezbqg3hD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reminders"
      ],
      "metadata": {
        "id": "wtdaWKValEel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Logistic Regression is a probabilistic method for solving supervised learning tasks. It can be **Binary Logistic Regression**, for classification tasks involving two classes only, or **Multi-Class Logistic Regression**, for classifications tasks involving more than two classes.\n",
        "\n",
        "It requires...\n",
        "\n",
        "... a **data set** of $N$ *input/output* pairs $(\\mathbf{x}^{(n)}, y^{(n)})$, for which the inputs are vectors $\\mathbf{x}^{(n)} = [x^{(n)}_1,x^{(n)}_2,...,x^{(n)}_D]$. \n",
        "\n",
        "*  For Binary LR, there is one output $y \\in [0,1]$ per input.\n",
        "*  For Multi-Class LR, there are $K$ outputs per input. One-hot encoding: $\\mathbf{y} = [y_1,y_2,...,y_K]$, for which $y_i \\in [0,1]$ and $\\sum_{i=1}^K y_i = 1$. For example, if input $\\mathbf{x}$ belongs to class two out of six classes, then $\\mathbf{y} = [0,1,0,0,0,0]$.\n",
        "\n",
        "... a **hypothesis class** or model that computes $\\hat{\\mathbf{y}}$ (estimated class probability).\n",
        "\n",
        "*  The sigmoid function ($\\sigma$) is used in Binary LR.  $$\\hat{y}=f(\\mathbf{x};\\mathbf{w},w_0) = \\sigma(\\mathbf{w}^\\top\\mathbf{x}+w_0) = \\frac{1}{1+e^{-\\mathbf{w}^\\top\\mathbf{x}+w_0}}.$$\n",
        "*  The softmax function is used in Multi-Class LR.\n",
        "$$\\hat{\\mathbf{y}}=f(\\mathbf{x};\\mathbf{W},\\mathbf{w}_0) =  \\text{softmax}(\\mathbf{w}_k^\\top\\mathbf{x}+w_0) = \\frac{\\exp(\\mathbf{w}_k^\\top\\mathbf{x}+w_0)}{\\sum_{j=1}^K\\exp(\\mathbf{w}_j^\n",
        "\\top\\mathbf{x}+w_0)}.$$\n",
        "\n",
        "... a **loss function** for learning, which involves minimizing the error on training examples  $\\rightarrow$ *Negative log-Likelihood or Cross-Entropy Error*.\n",
        "\n",
        "* For Binary LR.  $$\\mathcal{E}_\\mathcal{D}(\\mathbf{w},w_0) = -\\sum_{n=1}^{N}(y^{(n)}\\ln \\hat{y}^{(n)})+(1-y^{(n)})\\ln(1-\\hat{y}^{(n)}).$$\n",
        "* For Multi-Class LR.\n",
        "$$\\mathcal{E}_\\mathcal{D}(\\mathbf{W},\\mathbf{w}_0) = -\\sum_{n=1}^{N}\\sum_{k=1}^{K}y_k^{(n)}\\ln \\hat{\\mathbf{y}_k}^{(n)}.$$\n",
        "\n",
        "... an **optimization algorithm**\n",
        "\n",
        "---\n",
        "Gradient Descent\n",
        "---\n",
        "**Input**:\n",
        "- Dataset $\\mathcal{D}=\\{(\\mathbf{x}^{(n)}, y^{(n)})\\}_{n=1}^N$\n",
        "- Objective $J_\\mathcal{D}(\\mathbf{w})$\n",
        "- Gradient $\\nabla_\\mathbf{w} J_\\mathcal{D}(\\mathbf{w})$\n",
        "- Learning rate $\\eta>0$\n",
        "- Convergence threshold $\\epsilon>0$. \\\\\n",
        "\n",
        "*(assuming $\\mathbf{w}$ contains all parameters)* \n",
        "\n",
        "$\\mathbf{w}^{(0)} \\leftarrow \\mathbf{w}_\\text{init}$. *(Initialize weights)* \\\\\n",
        "$t \\leftarrow 0$\n",
        "\n",
        "**repeat** \n",
        "> $t \\leftarrow t+1$ \\\\\n",
        "> $\\mathbf{w}^{(t)} \\leftarrow \\mathbf{w}^{(t-1)} - \\eta \\nabla J_\\mathcal{D}(\\mathbf{w}^{(t-1)})$. *(Update weights)* \\\\\n",
        "\n",
        "**until** $|J_\\mathcal{D}(\\mathbf{w}^{(t)})-J_\\mathcal{D}(\\mathbf{w}^{(t-1)})|<\\epsilon  $ \\\\\n",
        "\n",
        "**return** $\\mathbf{w}^{(t)}$.\n",
        "\n",
        "---\n",
        "\n",
        "* Gradient for Binary LR.  $$\\nabla_\\mathbf{w} J_\\mathcal{D}(\\mathbf{w},w_0) = \\sum_{n=1}^{N}\\left(\\hat{y}^{(n)}-y^{(n)}\\right)\\mathbf{x}^{(n)}$$\n",
        "$$\\frac{\\partial J_\\mathcal{D}(\\mathbf{w},w_0)}{\\partial w_0} = \\sum_{n=1}^{N}\\left(\\hat{y}^{(n)}-y^{(n)}\\right)$$\n",
        "\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1.   **Training**: We minimize the loss function using Gradient Descent to train our model. \n",
        "2.   **Evaluation** of the model on test data: For test example $\\mathbf{x}$, we compute $p(y|\\mathbf{x})$ and return the label with highest probability, $y = 0$ or $y = 1$ (in the case of Binary LR).  \n",
        "\n"
      ],
      "metadata": {
        "id": "NZ_V3iM9kS5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Conceptual questions\n",
        "\n",
        "1. What is the classification criterion for a given point in Multi-Class Logistic Regression given the output of the model? \n",
        "2. Describe in words the relation between the sigmoid (or logistic) function and the softmax function used in binary or multi-class classification problems?\n"
      ],
      "metadata": {
        "id": "EnoC8KjHFMTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Analytical Exercises"
      ],
      "metadata": {
        "id": "MfYzXoEpzgc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 2.1. Consider a binary-class dataset: \n",
        "\n",
        "The data points are $\\{\\mathbf{x}^{(1)}=\\left(\\begin{smallmatrix}  1\\\\2\\end{smallmatrix}\\right),\\mathbf{x}^{(2)}=\\left(\\begin{smallmatrix}-1\\\\-2\\end{smallmatrix}\\right), \\mathbf{x}^{(3)}=\\left(\\begin{smallmatrix}  -2\\\\-1\\end{smallmatrix}\\right)\\}$ with labels $\\{y^{(1)}= 1,  y^{(2)}=0, y^{(3)}=0\\}$.\n",
        "\n",
        "* Write the parametric form of a logistic classifier $f(\\mathbf{x};\\mathbf{w},w_0)$.\n",
        "* Write the Negative Log-Likelihood function corresponding to this dataset using the logistic linear classifier, as a function of $\\mathbf{w}$ and $w_0$.\n"
      ],
      "metadata": {
        "id": "nnYPc8nU5hgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Consider three classes:\n",
        "\n",
        "Class 0: $\\{ \\mathbf{x}^{(1)}=\\left[\\begin{smallmatrix}0.5\\\\0.4\\end{smallmatrix}\\right] \\mathbf{x}^{(2)}=\\left[\\begin{smallmatrix}0.8\\\\0.3\\end{smallmatrix}\\right], \\mathbf{x}^{(3)}=\\left[\\begin{smallmatrix}0.3\\\\0.8\\end{smallmatrix}\\right]\\}$\n",
        "\n",
        "Class 1: $\\{ \\mathbf{x}^{(4)}=\\left[\\begin{smallmatrix}-0.4\\\\0.3\\end{smallmatrix}\\right], \\mathbf{x}^{(5)}=\\left[\\begin{smallmatrix}-0.3\\\\0.7\\end{smallmatrix}\\right], \\mathbf{x}^{(6)}=\\left[\\begin{smallmatrix}0.7\\\\0.2\\end{smallmatrix}\\right]\\}$\n",
        "\n",
        "Class 2: $\\{ \\mathbf{x}^{(7)}=\\left[\\begin{smallmatrix}0.7\\\\-0.4\\end{smallmatrix}\\right], \\mathbf{x}^{(8)}=\\left[\\begin{smallmatrix}0.5\\\\-0.6\\end{smallmatrix}\\right], \\mathbf{x}^{(9)}=\\left[\\begin{smallmatrix}-0.4\\\\-0.5\\end{smallmatrix}\\right]\\}$\n",
        "\n",
        "    \n",
        "And assume that the parameters of the three discriminants are: $\\mathbf{w}_0=\\left[\\begin{smallmatrix}-0.3\\\\0.87\\\\1.47\\end{smallmatrix}\\right]$, $\\mathbf{w}_1=\\left[\\begin{smallmatrix}-0.01\\\\0.58\\\\1.02\\end{smallmatrix}\\right]$ and $\\mathbf{w}_3=\\left[\\begin{smallmatrix}0.43\\\\-1.90\\\\0.33\\end{smallmatrix}\\right]$ \n",
        "\n",
        "\n",
        "1. Draw the points and use a 1-of-K representation of the output. \n",
        "2. Determine at which class the calculated softmax will classify the points $\\mathbf{x}^{(1)}$, $\\mathbf{x}^{(4)}$ and $\\mathbf{x}^{(7)}$.\n",
        "3. Calculate the error for the given solutions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z67P5f0s5leE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Coding exercise: Implementation\n",
        "\n",
        "We now will code the implementation of Binary Logistic Regression using gradient descent. We will use a *batch* approach. \n"
      ],
      "metadata": {
        "id": "R_M3IymZFbhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent: what is the intuition behind it?\n",
        "\n",
        "Let's play! https://www.i-am.ai/gradient-descent.html "
      ],
      "metadata": {
        "id": "Zp2wt2qv1csj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Binary Logistic Regression Gradient Descent Algorithm"
      ],
      "metadata": {
        "id": "R6_8Y5Kwk4ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import pandas as pd\n",
        "from scipy.special import expit as sigmoid\n",
        "import time"
      ],
      "metadata": {
        "id": "UJMXZtchinmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class my_LogisticRegression():\n",
        "    \"\"\"\n",
        "    Logistic Regression implemented with batch Gradient Descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_features = 0\n",
        "        self.n_samples = 0\n",
        "\n",
        "        # parameters for the decision rule\n",
        "        self.w = np.zeros([self.n_features+1]) # the intercept is included in w\n",
        "        self.nb_iter = 1\n",
        "\n",
        "        self.training_error = np.array([])\n",
        "    \n",
        "    def cross_entropy_error(self, X, y, w):\n",
        "        \"\"\"\n",
        "        Computes the Cross-Entropy Loss Function\n",
        "    \n",
        "        Parameters\n",
        "        ----------\n",
        "        w: ndarray, shape(1, n_features)\n",
        "        \n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points (e.g. each sample is in R^2).\n",
        "    \n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "    \n",
        "        Returns\n",
        "        -------\n",
        "        error: integer,\n",
        "                The Cross-Entropy Loss\n",
        "        \"\"\"\n",
        "\n",
        "        error = 0\n",
        "        for k in range(X.shape[0]):\n",
        "            yhat = sigmoid(self.w@X[k,:])\n",
        "\n",
        "            #TO DO\n",
        "            #compute negative log-likelihood / cross-entropy error for this point\n",
        "            CE_error =\n",
        "\n",
        "            error += CE_error\n",
        "\n",
        "        return -1*error\n",
        "\n",
        "    def gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the Cross-Entropy Loss Function\n",
        "    \n",
        "        Parameters\n",
        "        ----------\n",
        "        w: ndarray, shape(1, n_features)\n",
        "        \n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points (e.g. each sample is in R^2).\n",
        "    \n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "    \n",
        "        Returns\n",
        "        -------\n",
        "        grad: ndarray, shape (1, n_features)\n",
        "                The gradient of the Cross-Entropy Loss Function\n",
        "        \"\"\"\n",
        "\n",
        "        #initialize gradient\n",
        "        grad = np.zeros([self.n_features+1])\n",
        "        \n",
        "        for k in range(X.shape[0]):\n",
        "            yhat = sigmoid(self.w@X[k,:])\n",
        "\n",
        "            #TO DO\n",
        "            #compute gradient for this point\n",
        "            gradient_k = \n",
        "\n",
        "            grad += gradient_k \n",
        "        \n",
        "        return grad\n",
        "\n",
        "    def fit(self, X, y, max_iter = 10000, eps = 1E-7, learning_rate = 1E-6):\n",
        "        \"\"\"\n",
        "        Fit our Logistic Regression model to the data set (X, y)\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The training set.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points (e.g. each sample is in R^2).\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The class of each point (0: negative, 1: positive).\n",
        "            n_samples == number of points in the dataset.\n",
        "        Returns\n",
        "        -------\n",
        "        self: logReg\n",
        "               The Logistic Regression model trained on (X, y).\n",
        "        \"\"\"\n",
        "        \n",
        "        self.n_samples = X.shape[0]\n",
        "        self.n_features = X.shape[1]\n",
        "        X = np.append(X, np.ones((self.n_samples,1)), axis=1)\n",
        "        self.w = np.zeros([1, self.n_features + 1])[0]\n",
        "\n",
        "        cond = np.inf  # start with cond greater than eps \n",
        "        \n",
        "        while cond > eps and self.nb_iter < max_iter:\n",
        "            w_old = self.w\n",
        "\n",
        "            #TO DO\n",
        "            #update weights\n",
        "            self.w = \n",
        "\n",
        "            #check convergence condition\n",
        "            cond = np.linalg.norm(self.w - w_old)\n",
        "\n",
        "            #TO DO\n",
        "            #compute cross entropy error for this iteration and save it \n",
        "            error = \n",
        "\n",
        "            self.training_error = np.append(self.training_error, error)\n",
        "            self.nb_iter += 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the classes of points in X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The set of points whose classes are to predict.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points (e.g. each sample is in R^2).\n",
        "        Returns\n",
        "        -------\n",
        "        predictions: ndarray, shape (n_samples)\n",
        "                      The predicted classes.\n",
        "                      n_samples == number of points in the dataset.\n",
        "        \"\"\"\n",
        "        X = np.append(X, np.ones((X.shape[0],1)), axis=1)       \n",
        "        \n",
        "        #TO DO\n",
        "        # return prediction of the model for given X data points\n",
        "        prediction = \n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def classification_function(self, X):\n",
        "        X = np.append(X, np.ones((X.shape[0],1)), axis=1)\n",
        "        return np.reshape(sigmoid(self.w@X.T),(-1,))\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute the accuracy of the classifier on the set X, provided the ground-truth y.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: ndarray, shape (n_samples, n_features)\n",
        "            The set of points on which to compute the score.\n",
        "            n_samples == number of points in the dataset.\n",
        "            n_features == dimension of the points (e.g. each sample is in R^2).\n",
        "        y: ndarray, shape (n_samples,)\n",
        "            The ground-truth of the labels.\n",
        "            n_samples == number of points in the dataset.\n",
        "        Returns\n",
        "        -------\n",
        "        score: float\n",
        "                Accuracy of the classifier.\n",
        "        \"\"\"\n",
        "        return np.mean(self.predict(X) == y)"
      ],
      "metadata": {
        "id": "YJQc3ChZnAi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see a simple example, you can use it to test if  the code is working well"
      ],
      "metadata": {
        "id": "rBl6Vpo7QnWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy of the Logistic Regression\n",
        "X_train=np.array([[1.,1.,2.], [1.,-1.,-2.], [1.,-2.,-1.]])    \n",
        "y_train=np.array([1,0,0])\n",
        "clf = my_LogisticRegression()\n",
        "clf.fit(X_train, y_train, learning_rate = 0.1)\n",
        "print(f'The accuracy of the logistic regression on the training set is {clf.score(X_train,y_train)}')\n",
        "plt.title('Convergence')\n",
        "plt.plot(clf.training_error)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cross-Entropy training error')"
      ],
      "metadata": {
        "id": "hbSrT-zyxez5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Dataset"
      ],
      "metadata": {
        "id": "HW-3LVpslRIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will work with the same dataset used in the previous session: **Titanic - Machine Learning from Disaster** Dataset from the following kaggle competition : https://www.kaggle.com/competitions/titanic/data. "
      ],
      "metadata": {
        "id": "XkRFQt_SZiCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should upload to the google colab(or your local machine) the file train.csv that you can find on Aula Global. We start by reading the file, we can use the read_csv method to obtain a Dataframe a python object containing the content of the csv file. We can use the **head** method to see the first rows of the data."
      ],
      "metadata": {
        "id": "9Dj4BqmDZ7NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "train.rename(columns = {'Sex':'Gender'}, inplace= True)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "Pb25fVecFbPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select the following features: \n",
        "- `Pclass` : The class of the ticket, it can be 1rd, 2nd or 3rd\n",
        "- `Gender` : The gender of the Passenger\n",
        "- `Age` : Age in Years\n",
        "- `Fare` : Passenger Fare\n",
        "- `embarked` : Port of Embarkation(C = Cherbourg, Q = Queenstown, S= Southampton)\n",
        "- `Survived`: If the passenger survived (1) or not (0)"
      ],
      "metadata": {
        "id": "8z-Uh3UdaQt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = train.loc[:,['Pclass', 'Gender', 'Age', 'Fare','Embarked','Survived']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_gH6dZ82wxe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that the value of the Age is missing for 177 passengers, we replace it by the mean value of the age of the passenger.(This is a solution, but not the only one). The value of Embarked is missing for two passengers, we select the most common value for them)"
      ],
      "metadata": {
        "id": "Ayl4XzXuapWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "hIFnatX9awcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Age.fillna(df.Age.mean(),inplace=True)\n",
        "df.Embarked.fillna(df.Embarked.loc[df.Embarked.value_counts().argmax()], inplace=True)\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "tJBEJbLawyyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember encoding from the past session? Let's review it again!"
      ],
      "metadata": {
        "id": "cxdHebw1ZdQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NUMERIC Encoding of the Passenger class\n",
        "Pclass = df.Pclass.values / 3\n",
        "Pclass_enc = Pclass.reshape(-1,1)\n",
        "\n",
        "#TO DO\n",
        "# ONE-HOT Encoding of the Gender\n",
        "Gender = df.Gender.values\n",
        "print('Format before encoding', Gender[0:5])\n",
        "Gender_enc =\n",
        "print('Format after encoding', Gender_enc[0:5])\n",
        "\n",
        "# ONE-HOT Encoding of the Port of Embarkation\n",
        "embarked = df.Embarked.values\n",
        "print('Format before encoding', embarked[0:5])\n",
        "labels = np.unique(embarked)\n",
        "embarked_enc = np.zeros((len(embarked), len(labels)))\n",
        "for i, l in enumerate(labels):\n",
        "    embarked_enc[:,i] = embarked == l\n",
        "embarked_enc = embarked_enc.astype('float')\n",
        "print('Format after encoding', embarked_enc[0:5])"
      ],
      "metadata": {
        "id": "RTwbtyh8w3Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the numerical features and the target from the dataframe\n",
        "X = df.loc[:,['Age', 'Fare']].values\n",
        "y = df.loc[:,'Survived'].values"
      ],
      "metadata": {
        "id": "eE8Wym4-w-Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the encoded numerical variables\n",
        "X_aug = np.hstack([X, Pclass_enc, Gender_enc, embarked_enc])"
      ],
      "metadata": {
        "id": "R9qC-d1Aw_43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to evaluate the quality of our model, we could use it to make predictions on the test set, but we don't have access to the true values on this set(we could make a prediction on kaggle to get a scoring). As done in the previous seminar, one way to get around this is to split the training data into training and validation data to evaluate the performance of the model on data it has never seen."
      ],
      "metadata": {
        "id": "Tm3DBgy-baR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO\n",
        "#separate dataset into training and validation sets (use train_test_split function)\n",
        "id_train, id_val, y_train, y_val =\n",
        "\n",
        "# Select the training samples from the dataset containing only Age and Fare\n",
        "X_train = X[id_train]\n",
        "X_val = X[id_val]\n",
        "print(f' Training set size : {len(id_train)}\\n Validation set size : {len(id_val)}')\n",
        "\n",
        "# Select the same training samples for the training dataset with the encoded numerical variables\n",
        "X_train_aug = X_aug[id_train]\n",
        "X_val_aug = X_aug[id_val]"
      ],
      "metadata": {
        "id": "hcR1fp6MxDGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Application\n"
      ],
      "metadata": {
        "id": "vD81GlcVFzLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply our model to the dataset that only contains the variables Age and Fare."
      ],
      "metadata": {
        "id": "GTQdkyn7cA3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy of the Logistic Regression (2 features)\n",
        "\n",
        "#TO DO\n",
        "#Train the model with the non-augmented data (max_iter = 5000)\n",
        "\n",
        "\n",
        "print(f'The accuracy of the logistic regression on the training set is {clf.score(X_train,y_train)}')\n",
        "print(f'The accuracy of the logistic regression on the validation set is {(y_pred == y_val).mean()}')\n",
        "plt.title('Convergence')\n",
        "plt.plot(clf.training_error)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cross-Entropy training error')"
      ],
      "metadata": {
        "id": "W67nlmU87G2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's apply our model to the dataset that also contains the encoded numerical variables."
      ],
      "metadata": {
        "id": "bvID1vO2cU3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy of the Logistic Regression (augmented)\n",
        "\n",
        "#TO DO\n",
        "#Train the model with the augmented data (max_iter = 5000)\n",
        "\n",
        "\n",
        "print(f'The accuracy of the logistic regression on the training set is {clf.score(X_train_aug,y_train)}')\n",
        "print(f'The accuracy of the logistic regression on the validation set is {(y_pred == y_val).mean()}')\n",
        "plt.title('Convergence')\n",
        "plt.plot(clf.training_error)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cross-Entropy training error')"
      ],
      "metadata": {
        "id": "C06-k-LYcUpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Does the addition of encoded variables help the model? Is there a difference in the convergence speed?  Do we need to tune the number of iterations?"
      ],
      "metadata": {
        "id": "BxKcINZsXnCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning rate, convergence and speed"
      ],
      "metadata": {
        "id": "3rw3u_Kgcqt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code, we will see how the choice of the learning rate parameter can affect the convergence and speed."
      ],
      "metadata": {
        "id": "hRmhcSQ01LPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(8, figsize = (10,15))\n",
        "\n",
        "for i in np.arange(8):    \n",
        "    lr = 1/10**(i+1)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    #TO DO\n",
        "    #Train our model on the non augmented dataset with the corresponding \n",
        "    # learning rate (lr) and max_iter = 1000\n",
        "\n",
        "\n",
        "    #Assess performance\n",
        "    t2 = time.perf_counter()\n",
        "    print('Learning rate',lr,'.Time taken to run:',t2-t1,'. Number of iterations', clf.nb_iter)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    axs[i].plot(clf.training_error,label = f'learning rate:{lr}')\n",
        "    axs[i].set_title(f'learning rate:{lr}')\n",
        "    axs[i].set_ylabel('C-E Loss')\n",
        "    axs[i].set_xlabel('Iteration')\n",
        "\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "fh39tXEEzSO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: For which values of the learning rate we find convergence?\n",
        "\n",
        "Q: Which value of the learning rate do you think is optimal?\n",
        "\n",
        "Q: What happens for the larger values of the learning rate? \n",
        "\n",
        "Q: What happens for the smaller values of the learning rate? \n"
      ],
      "metadata": {
        "id": "ViyNWNsCAYt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Sklearn Library: how to make your life easier\n"
      ],
      "metadata": {
        "id": "RHYPkqEBYvAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you saw in the previous session, there are libraries with off the shelf implementations of these models. In this case we will use the module LogisticRegression from sklearn. "
      ],
      "metadata": {
        "id": "Dj7UNspx-fWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Accuracy of the augmented model\n",
        "\n",
        "# We select the parameters that fit our approach better in order to be \"fair\" to our implementation: NO regularization!\n",
        "clf = LogisticRegression(multi_class = 'multinomial', penalty = None, max_iter=1000, solver='saga')\n",
        "\n",
        "#TO DO\n",
        "#Fit the model with the augmented dataset and find the accuracy of the training and validation set\n",
        "\n",
        "\n",
        "print(f'The accuracy of the logistic regression on the training set is {accuracy_training}')\n",
        "print(f'The accuracy of the logistic regression with the augmented model on the validation set is {accuracy_validation}')"
      ],
      "metadata": {
        "id": "r20SfkrpYuIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have been computing two accuracy values: one for the training dataset and one for the validation (or testing) set. \n",
        "\n",
        "Q: Which one is larger? Why?\n",
        "\n",
        "Q: Why isn't it enough to check the accuracy on the testing set?"
      ],
      "metadata": {
        "id": "x5M27aCGB6H4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zq7ppsFINyoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}